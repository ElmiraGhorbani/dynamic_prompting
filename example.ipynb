{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Prompting\n",
    "\n",
    "This example goes over how to use DynamicPrompting to interact with dynamic few-shot prompting inference for downstream tasks such as classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dynamic_prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ElmiraGhorbani/dynamic_prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dynamic_prompting.embedings.config import EmbeddingsConfig\n",
    "from dynamic_prompting.knowledge_base.config import KnowledgeBaseConfig\n",
    "from dynamic_prompting.knowledge_base.knowledge_base import \\\n",
    "    KnowledgeBaseManagement\n",
    "from dynamic_prompting.llms.config import LLMConfig, PromptConfig\n",
    "from dynamic_prompting.llms.llm import LlamaModel\n",
    "from dynamic_prompting.llms.prompt import PromptManagement\n",
    "from dynamic_prompting.utils.utils import get_project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the following options, just uncomment one.\n",
    "\n",
    "# Nomic: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n",
    "emb_config = EmbeddingsConfig(\n",
    "    model_name=\"nomic-embed-text-v1.5\",\n",
    "    local_files=True,\n",
    "    trust_remote_code=True,\n",
    "    max_dimension=512,\n",
    ")\n",
    "\n",
    "\n",
    "# Mxbai: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "\n",
    "# emb_config = EmbeddingsConfig(\n",
    "#     model_name=\"mxbai-embed-large-v1\",\n",
    "#     local_files=True,\n",
    "#     trust_remote_code=False,\n",
    "#     max_dimension=512,\n",
    "# )\n",
    "\n",
    "\n",
    "# BGE : https://huggingface.co/BAAI/bge-small-en-v1.5\n",
    "# emb_config = EmbeddingsConfig(\n",
    "#     model_name=\"bge-small-en-v1.5\",\n",
    "#     local_files=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowledge Base Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_config = KnowledgeBaseConfig(\n",
    "    knowledge_base_name='nomic_text_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=8,\n",
    "    local_files=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is for the classification case\n",
    "\n",
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "You are given a text sample, and your task is to classify it into one of the following categories: Business, Science, Sports,\\n\n",
    "Politics, Entertainment, Weather, Technology, Health, Local, World, Culture, Education, Travel.\\n\n",
    "Carefully read the text and determine the most appropriate category that best describes the main topic of the text.\\n\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "**Text:** \"Your text sample here\"\n",
    "**Category:** \"Only return the category\"\n",
    "\"\"\"\n",
    "prompt_config = PromptConfig(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBaseManagement(embeddings_config=emb_config, kb_config=kb_config)\n",
    "\n",
    "llama_model = LlamaModel(llm_config=llm_config)\n",
    "\n",
    "\n",
    "prompt_handler = PromptManagement(prompt_config=prompt_config)\n",
    "\n",
    "root_path = get_project_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data folder to learn about the data structure for dynamic few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    f'{root_path}/data/sample.csv',\n",
    "    index_col=False\n",
    ")\n",
    "messages = df['txt'].tolist()\n",
    "messages = [i.replace('\\n', ' ') for i in messages]\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.create_kb(messages)\n",
    "\n",
    "db = kb.load_kb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Dynamic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''The city council has announced a new initiative to improve public transportation\\\n",
    "    infrastructure, aiming to reduce traffic congestion and promote eco-friendly travel options.'''\n",
    "\n",
    "indx = kb.search_kb(\n",
    "    query=query,\n",
    "    embeddings=db,\n",
    "    num_of_neighbours=5\n",
    ")\n",
    "\n",
    "\n",
    "samples = [messages[i] for i in indx]\n",
    "labels = [labels[i] for i in indx]\n",
    "\n",
    "prompt = prompt_handler.set_few_shots(context_input=samples, labels=labels)\n",
    "\n",
    "# run model prediction\n",
    "inf_result = llama_model.inference(\n",
    "    system_instruction=prompt,\n",
    "    user_input=query\n",
    ")\n",
    "\n",
    "print(inf_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
